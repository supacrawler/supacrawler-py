{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium vs SupaCrawler: Complete Performance Comparison\n",
    "\n",
    "This notebook compares Selenium WebDriver against SupaCrawler for both single-page scraping and multi-page crawling. We focus on performance, resource usage, and infrastructure complexity.\n",
    "\n",
    "Note: this test was done on a local Mac M4 machine with 24GB Memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation requirements\n",
    "# !pip install selenium webdriver-manager\n",
    "# !pip install supacrawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from supacrawler import SupacrawlerClient\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Single Page Scraping Comparison\n",
    "For the first comparison, let's try rendering javascript from both SDKs. We'll run both Selenium & Supacrawler locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Page Scraping Comparison\n",
      "===================================\n",
      "Test URL: https://supabase.com\n",
      "\n",
      "Selenium WebDriver:\n",
      "Title: Supabase | The Postgres Development Platform.\n",
      "Time: 4.08s\n",
      "\n",
      "SupaCrawler:\n",
      "Title: Supabase | The Postgres Development Platform.\n",
      "Time: 1.37s\n",
      "\n",
      "Performance: SupaCrawler is 3.0x faster\n"
     ]
    }
   ],
   "source": [
    "def selenium_scrape(url):\n",
    "    \"\"\"Single page scraping with Selenium\"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    # Setup Chrome options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    # Setup Chrome driver\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(2)  # Wait for page load\n",
    "        \n",
    "        title = driver.title\n",
    "        body = driver.find_element(By.TAG_NAME, \"body\")\n",
    "        content = body.text[:200] + \"...\" if len(body.text) > 200 else body.text\n",
    "        \n",
    "        result = {\n",
    "            'title': title,\n",
    "            'content': content,\n",
    "            'time': time.time() - start,\n",
    "            'javascript_support': True,\n",
    "            'resource_usage': 'High (local browser)'\n",
    "        }\n",
    "        \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return result\n",
    "\n",
    "def supacrawler_scrape(url):\n",
    "    \"\"\"Single page scraping with SupaCrawler\"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    client = SupacrawlerClient(api_key='')\n",
    "    response = client.scrape(url, format='markdown', render_js=True, fresh=True)\n",
    "    \n",
    "    title = response.metadata.title if response.metadata else 'No title'\n",
    "    content = response.content if response.content else \"No content\"\n",
    "    \n",
    "    return {\n",
    "        'title': title,\n",
    "        'content': content[:200] + \"...\" if len(content) > 200 else content,\n",
    "        'metadata': response.metadata,\n",
    "        'time': time.time() - start,\n",
    "        'javascript_support': True,\n",
    "        'resource_usage': 'Zero local resources'\n",
    "    }\n",
    "\n",
    "# Test single page scraping\n",
    "test_url = 'https://supabase.com'\n",
    "\n",
    "print(\"Single Page Scraping Comparison\")\n",
    "print(\"=\" * 35)\n",
    "print(f\"Test URL: {test_url}\")\n",
    "print()\n",
    "\n",
    "print(\"Selenium WebDriver:\")\n",
    "selenium_result = selenium_scrape(test_url)\n",
    "print(f\"Title: {selenium_result['title']}\")\n",
    "print(f\"Time: {selenium_result['time']:.2f}s\")\n",
    "print()\n",
    "\n",
    "print(\"SupaCrawler:\")\n",
    "sc_result = supacrawler_scrape(test_url)\n",
    "print(f\"Title: {sc_result['title']}\")\n",
    "print(f\"Time: {sc_result['time']:.2f}s\")\n",
    "print()\n",
    "\n",
    "if selenium_result['time'] > 0 and sc_result['time'] > 0:\n",
    "    ratio = selenium_result['time'] / sc_result['time']\n",
    "    print(f\"Performance: SupaCrawler is {ratio:.1f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Multi-Page Crawling Comparison\n",
    "\n",
    "### How SupaCrawler Crawl Works\n",
    "\n",
    "1. **Crawl dispatch** → Crawl breaks the target domain into a queue of page URLs.\n",
    "2. **Per-page scrape** → For each URL, crawl calls the exact same **scrape service** that is exposed publicly (`/scrape`).\n",
    "3. **Retry & backoff** → Each scrape runs with up to **3 retries** and **exponential backoff** (1s → 2s → 4s).\n",
    "4. **Error handling** → If all retries fail, the page is marked as failed.\n",
    "5. **Aggregation** → Successfully scraped pages are aggregated into the crawl result.\n",
    "\n",
    "To make the Selenium and Playwright benchmarks fair, we applied the **same retry + backoff algorithm** when fetching each page.\n",
    "\n",
    "The crawl path is literally just a loop of our scrape service. That means the **retry, backoff, and error logic** are identical between crawl and scrape. To make the Selenium and Playwright benchmarks fair, we applied the **same retry + backoff algorithm** when fetching each page.\n",
    "\n",
    "Therefore, **all three systems (SupaCrawler, Selenium, Playwright)** are operating under identical conditions — same error tolerance, same retry count, same backoff schedule.\n",
    "\n",
    "This guarantees the benchmark is **apples-to-apples**: differences come from **architecture and performance**, not from error-handling or retry policies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import WebDriverException\n",
    "\n",
    "def selenium_crawl(start_url, max_pages=5, max_retries=3):\n",
    "    \"\"\"Manual crawling with Selenium WebDriver + retries/backoff\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    \n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    \n",
    "    visited = set()\n",
    "    to_visit = deque([start_url])\n",
    "    results = []\n",
    "    \n",
    "    try:\n",
    "        while to_visit and len(results) < max_pages:\n",
    "            url = to_visit.popleft()\n",
    "            if url in visited:\n",
    "                continue\n",
    "            visited.add(url)\n",
    "\n",
    "            attempt = 0\n",
    "            success = False\n",
    "\n",
    "            while attempt < max_retries and not success:\n",
    "                try:\n",
    "                    driver.get(url)\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "                    )\n",
    "                    time.sleep(2)  # let JS render\n",
    "\n",
    "                    title = driver.title or \"N/A\"\n",
    "                    body = driver.find_element(By.TAG_NAME, \"body\")\n",
    "                    content = body.text[:500] + \"...\" if len(body.text) > 500 else body.text\n",
    "\n",
    "                    # Meta tags\n",
    "                    try:\n",
    "                        description = driver.find_element(By.XPATH, \"//meta[@name='description']\").get_attribute(\"content\")\n",
    "                    except:\n",
    "                        description = None\n",
    "                    try:\n",
    "                        keywords = driver.find_element(By.XPATH, \"//meta[@name='keywords']\").get_attribute(\"content\")\n",
    "                    except:\n",
    "                        keywords = None\n",
    "\n",
    "                    # Headers\n",
    "                    headers = []\n",
    "                    for tag in [\"h1\", \"h2\", \"h3\"]:\n",
    "                        for elem in driver.find_elements(By.TAG_NAME, tag):\n",
    "                            text = elem.text.strip()\n",
    "                            if text:\n",
    "                                headers.append({tag: text})\n",
    "\n",
    "                    # Links (same domain only)\n",
    "                    base_domain = urlparse(start_url).netloc\n",
    "                    links = []\n",
    "                    try:\n",
    "                        link_elements = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                        for link in link_elements:\n",
    "                            href = link.get_attribute(\"href\")\n",
    "                            if href and href.startswith(\"http\"):\n",
    "                                if urlparse(href).netloc == base_domain:\n",
    "                                    links.append(href)\n",
    "                                    if href not in visited and len(to_visit) < 20:\n",
    "                                        to_visit.append(href)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    # Metadata\n",
    "                    metadata = {\n",
    "                        \"url\": url,\n",
    "                        \"title\": title,\n",
    "                        \"description\": description,\n",
    "                        \"keywords\": keywords,\n",
    "                        \"headers\": headers,\n",
    "                        \"word_count\": len(body.text.split()),\n",
    "                        \"links_found\": len(links)\n",
    "                    }\n",
    "\n",
    "                    results.append({\n",
    "                        \"content\": content,\n",
    "                        \"metadata\": metadata\n",
    "                    })\n",
    "\n",
    "                    # Success, mark and stop retry loop\n",
    "                    success = True\n",
    "                    # Random sleep to avoid fixed pattern\n",
    "                    time.sleep(random.uniform(1, 2))\n",
    "\n",
    "                except WebDriverException as e:\n",
    "                    attempt += 1\n",
    "                    if attempt < max_retries:\n",
    "                        backoff = 2 ** (attempt - 1)  # 1s, 2s, 4s\n",
    "                        print(f\"Retry {attempt} for {url} after {backoff}s due to {e}\")\n",
    "                        time.sleep(backoff)\n",
    "                    else:\n",
    "                        print(f\"Failed to scrape {url} after {max_retries} attempts\")\n",
    "                        break\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return {\n",
    "        \"pages_crawled\": len(results),\n",
    "        \"total_time\": end_time - start_time,\n",
    "        \"avg_time_per_page\": (end_time - start_time) / len(results) if results else 0,\n",
    "        \"results\": results\n",
    "    }\n",
    "\n",
    "def supacrawler_crawl(start_url, max_pages=5):\n",
    "    \"\"\"Built-in crawling with SupaCrawler (SDK-native usage)\"\"\"\n",
    "    start_time = time.time()\n",
    "    client = SupacrawlerClient(api_key=\"\")\n",
    "\n",
    "    try:\n",
    "        job = client.create_crawl_job(\n",
    "            url=start_url,\n",
    "            format=\"markdown\",\n",
    "            link_limit=max_pages,\n",
    "            depth=3,\n",
    "            include_subdomains=False,\n",
    "            render_js=True,\n",
    "            fresh=True # fresh never uses cached results\n",
    "        )\n",
    "\n",
    "        crawl_output = client.wait_for_crawl(\n",
    "            job.job_id,\n",
    "            interval_seconds=1.0,\n",
    "            timeout_seconds=120.0\n",
    "        )\n",
    "\n",
    "        crawl_data = crawl_output.data.crawl_data\n",
    "        end_time = time.time()\n",
    "\n",
    "        return {\n",
    "            \"pages_crawled\": len(crawl_data),\n",
    "            \"total_time\": end_time - start_time,\n",
    "            \"avg_time_per_page\": (end_time - start_time) / len(crawl_data) if crawl_data else 0,\n",
    "            \"crawl_data\": crawl_data  # keep the native objects\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        end_time = time.time()\n",
    "        return {\n",
    "            \"pages_crawled\": 0,\n",
    "            \"total_time\": end_time - start_time,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selenium results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Page Crawling Comparison\n",
      "===================================\n",
      "Test URL: https://docs.python.org\n",
      "Max pages: 10\n",
      "\n",
      "Selenium manual crawling:\n",
      "Pages crawled: 10\n",
      "Total time: 42.11s\n",
      "Average per page: 4.21s\n",
      "\n",
      "\n",
      "First Selenium page result:\n",
      "URL: N/A\n",
      "Title: N/A\n",
      "Text Preview: dev (3.15)\n",
      "pre (3.14)\n",
      "3.13.7\n",
      "3 ...\n",
      "\n",
      "Metadata: {'url': 'https://docs.python.org', 'title': '3.13.7 Documentation', 'description': 'The official Python documentation.', 'keywords': None, 'headers': [{'h1': 'Python 3.13.7 documentation'}], 'word_count': 244, 'links_found': 70}\n"
     ]
    }
   ],
   "source": [
    "# Test crawling\n",
    "crawl_url = \"https://docs.python.org\"\n",
    "max_pages = 10\n",
    "\n",
    "print(\"Multi-Page Crawling Comparison\")\n",
    "print(\"=\" * 35)\n",
    "print(f\"Test URL: {crawl_url}\")\n",
    "print(f\"Max pages: {max_pages}\")\n",
    "print()\n",
    "\n",
    "# --- Selenium ---\n",
    "print(\"Selenium manual crawling:\")\n",
    "selenium_crawl_result = selenium_crawl(crawl_url, max_pages)\n",
    "print(f\"Pages crawled: {selenium_crawl_result['pages_crawled']}\")\n",
    "print(f\"Total time: {selenium_crawl_result['total_time']:.2f}s\")\n",
    "print(f\"Average per page: {selenium_crawl_result['avg_time_per_page']:.2f}s\")\n",
    "print()\n",
    "\n",
    "if selenium_crawl_result[\"pages_crawled\"] > 0:\n",
    "    first_page = selenium_crawl_result[\"results\"][0]\n",
    "    print(\"\\nFirst Selenium page result:\")\n",
    "    print(\"URL:\", first_page[\"metadata\"][\"url\"])\n",
    "    print(\"Title:\", first_page[\"metadata\"][\"title\"])\n",
    "    print(\"Text Preview:\", first_page.get(\"content\", \"\")[:30], \"...\\n\")\n",
    "    print(\"Metadata:\", first_page[\"metadata\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supacrawler results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SupaCrawler built-in crawling:\n",
      "Pages crawled: 10\n",
      "Total time: 2.05s\n",
      "Average per page: 0.20s\n",
      "\n",
      "First SupaCrawler page result:\n",
      "Markdown Preview: # Python 3.13.7 documentation\n",
      "Welcome! This is the official documentation for Python 3.13.7.\n",
      "**Documentation sections:**\n",
      "[What's new in Python 3.13?](whatsnew/3.13.html)\n",
      "Or [all \"What's new\" documents ...\n",
      "\n",
      "Metadata: {'title': '3.13.7 Documentation', 'status_code': 200, 'description': 'The official Python documentation.', 'canonical': 'https://docs.python.org/3/index.html', 'favicon': 'https://docs.python.org/_static/py.svg', 'og_title': 'Python 3.13 documentation', 'og_description': 'The official Python documentation.', 'og_image': 'https://docs.python.org/3/_static/og-image.png', 'source_url': 'https://docs.python.org'}\n"
     ]
    }
   ],
   "source": [
    "# --- SupaCrawler ---\n",
    "print(\"SupaCrawler built-in crawling:\")\n",
    "sc_crawl_result = supacrawler_crawl(crawl_url, max_pages)\n",
    "\n",
    "if \"error\" in sc_crawl_result:\n",
    "    print(f\"Error: {sc_crawl_result['error']}\")\n",
    "else:\n",
    "    print(f\"Pages crawled: {sc_crawl_result['pages_crawled']}\")\n",
    "    print(f\"Total time: {sc_crawl_result['total_time']:.2f}s\")\n",
    "    print(f\"Average per page: {sc_crawl_result['avg_time_per_page']:.2f}s\")\n",
    "\n",
    "    # Show preview of first page (SDK-native)\n",
    "    if sc_crawl_result[\"pages_crawled\"] > 0:\n",
    "        first_url = sc_crawl_result[\"crawl_data\"][0]\n",
    "        print(\"\\nFirst SupaCrawler page result:\")\n",
    "        print(\"Markdown Preview:\", first_url.markdown[:200], \"...\")\n",
    "        print(\"\\nMetadata:\", first_url.metadata.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare their performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Performance: SupaCrawler is 20.6x faster per page\n"
     ]
    }
   ],
   "source": [
    "# --- Performance summary ---\n",
    "if (\n",
    "    selenium_crawl_result[\"pages_crawled\"] > 0\n",
    "    and sc_crawl_result.get(\"pages_crawled\", 0) > 0\n",
    "    and \"error\" not in sc_crawl_result\n",
    "):\n",
    "    speed_ratio = selenium_crawl_result[\"avg_time_per_page\"] / sc_crawl_result[\"avg_time_per_page\"]\n",
    "    print(f\"\\n\\nPerformance: SupaCrawler is {speed_ratio:.1f}x faster per page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First SupaCrawler page result:\n",
      "Markdown Preview: # Python 3.13.7 documentation\n",
      "Welcome! This is the official documentation for Python 3.13.7.\n",
      "**Documentation sections:**\n",
      "[What's new in Python 3.13?](whatsnew/3.13.html)\n",
      "Or [all \"What's new\" documents ...\n",
      "\n",
      "Metadata: {'title': '3.13.7 Documentation', 'status_code': 200, 'description': 'The official Python documentation.', 'canonical': 'https://docs.python.org/3/index.html', 'favicon': 'https://docs.python.org/_static/py.svg', 'og_title': 'Python 3.13 documentation', 'og_description': 'The official Python documentation.', 'og_image': 'https://docs.python.org/3/_static/og-image.png', 'source_url': 'https://docs.python.org'}\n"
     ]
    }
   ],
   "source": [
    "if sc_crawl_result[\"pages_crawled\"] > 0:\n",
    "    first_url = sc_crawl_result[\"crawl_data\"][0]\n",
    "    print(\"\\nFirst SupaCrawler page result:\")\n",
    "    print(\"Markdown Preview:\", first_url.markdown[:200], \"...\")\n",
    "    print(\"\\nMetadata:\", first_url.metadata.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: A more comprehensive test for crawling multiple pages!\n",
    "Let's try running for 50 pages on 3 different websites this time:\n",
    "-  supacrawler.com\n",
    "-  supabase.com\n",
    "-  ai.google.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Site Crawling Comparison\n",
      "\n",
      "\n",
      "============================================================\n",
      "Benchmarking: https://supabase.com\n",
      "Max pages: 50\n",
      "============================================================\n",
      "\n",
      "[Selenium] Manual crawling:\n",
      "  Pages crawled: 50\n",
      "  Total time: 241.49s\n",
      "  Avg per page: 4.83s\n",
      "  First page title: Supabase | The Postgres Development Platform.\n",
      "\n",
      "[SupaCrawler] Built-in crawling:\n",
      "  Pages crawled: 50\n",
      "  Total time: 36.26s\n",
      "  Avg per page: 0.73s\n",
      "  First page markdown preview: # Build in a weekendScale to millions\n",
      "Supabase is the Postgres development platform.\n",
      "Start your project with a Postgres  ...\n",
      "\n",
      "⚡ Performance: SupaCrawler is 6.7x faster per page\n",
      "\n",
      "============================================================\n",
      "Benchmarking: https://docs.python.org\n",
      "Max pages: 50\n",
      "============================================================\n",
      "\n",
      "[Selenium] Manual crawling:\n",
      "  Pages crawled: 50\n",
      "  Total time: 206.45s\n",
      "  Avg per page: 4.13s\n",
      "  First page title: 3.13.7 Documentation\n",
      "\n",
      "[SupaCrawler] Built-in crawling:\n",
      "  Pages crawled: 50\n",
      "  Total time: 41.33s\n",
      "  Avg per page: 0.83s\n",
      "  First page markdown preview: # Python 3.13.7 documentation\n",
      "Welcome! This is the official documentation for Python 3.13.7.\n",
      "**Documentation sections:** ...\n",
      "\n",
      "⚡ Performance: SupaCrawler is 5.0x faster per page\n",
      "\n",
      "============================================================\n",
      "Benchmarking: https://ai.google.dev\n",
      "Max pages: 50\n",
      "============================================================\n",
      "\n",
      "[Selenium] Manual crawling:\n",
      "  Pages crawled: 50\n",
      "  Total time: 455.67s\n",
      "  Avg per page: 9.11s\n",
      "  First page title: Gemini Developer API | Gemma open models  |  Google AI for Developers\n",
      "\n",
      "[SupaCrawler] Built-in crawling:\n",
      "  Pages crawled: 50\n",
      "  Total time: 37.25s\n",
      "  Avg per page: 0.74s\n",
      "  First page markdown preview: [NewGemini 2.5 Flash Image (aka Nano Banana) is now available in the Gemini API!](https://ai.google.dev/gemini-api/docs/ ...\n",
      "\n",
      "⚡ Performance: SupaCrawler is 12.2x faster per page\n"
     ]
    }
   ],
   "source": [
    "# Benchmark Runner: Compare Selenium vs SupaCrawler across multiple sites\n",
    "test_sites = [\n",
    "    \"https://supabase.com\",\n",
    "    \"https://docs.python.org\",\n",
    "    \"https://ai.google.dev\",\n",
    "]\n",
    "max_pages = 50\n",
    "\n",
    "def run_comparison(crawl_url: str, max_pages: int):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"Benchmarking: {crawl_url}\")\n",
    "    print(f\"Max pages: {max_pages}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # --- Selenium ---\n",
    "    print(\"\\n[Selenium] Manual crawling:\")\n",
    "    selenium_result = selenium_crawl(crawl_url, max_pages)\n",
    "    print(f\"  Pages crawled: {selenium_result['pages_crawled']}\")\n",
    "    print(f\"  Total time: {selenium_result['total_time']:.2f}s\")\n",
    "    print(f\"  Avg per page: {selenium_result['avg_time_per_page']:.2f}s\")\n",
    "\n",
    "    if selenium_result[\"pages_crawled\"] > 0:\n",
    "        first_page = selenium_result[\"results\"][0]\n",
    "        print(\"  First page title:\", first_page[\"metadata\"][\"title\"])\n",
    "\n",
    "    # --- SupaCrawler ---\n",
    "    print(\"\\n[SupaCrawler] Built-in crawling:\")\n",
    "    sc_result = supacrawler_crawl(crawl_url, max_pages)\n",
    "\n",
    "    if \"error\" in sc_result:\n",
    "        print(f\"  Error: {sc_result['error']}\")\n",
    "    else:\n",
    "        print(f\"  Pages crawled: {sc_result['pages_crawled']}\")\n",
    "        print(f\"  Total time: {sc_result['total_time']:.2f}s\")\n",
    "        print(f\"  Avg per page: {sc_result['avg_time_per_page']:.2f}s\")\n",
    "\n",
    "        if sc_result[\"pages_crawled\"] > 0:\n",
    "            first_url = sc_result[\"crawl_data\"][0]\n",
    "            print(\"  First page markdown preview:\", first_url.markdown[:120], \"...\")\n",
    "\n",
    "    # --- Performance summary ---\n",
    "    if (\n",
    "        selenium_result[\"pages_crawled\"] > 0\n",
    "        and sc_result.get(\"pages_crawled\", 0) > 0\n",
    "        and \"error\" not in sc_result\n",
    "    ):\n",
    "        ratio = selenium_result[\"avg_time_per_page\"] / sc_result[\"avg_time_per_page\"]\n",
    "        print(f\"\\n⚡ Performance: SupaCrawler is {ratio:.1f}x faster per page\")\n",
    "\n",
    "# --- Run benchmark for all test sites ---\n",
    "print(\"Multi-Site Crawling Comparison\\n\")\n",
    "for site in test_sites:\n",
    "    run_comparison(site, max_pages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Site Crawling Comparison\n",
    "\n",
    "| Site | Tool        | Pages Crawled | Total Time (s) | Avg per Page (s) | First Page Title / Preview | Performance Gain |\n",
    "|------|------------|---------------|----------------|------------------|----------------------------|------------------|\n",
    "| [Supabase](https://supabase.com) | **Selenium**     | 50 | 241.49 | 4.83 | *Supabase \\| The Postgres Development Platform.* | — |\n",
    "|      | **SupaCrawler** | 50 | 36.26  | 0.73 | `# Build in a weekend — Scale to millions` … | ⚡ **6.7× faster** |\n",
    "| [Python Docs](https://docs.python.org) | **Selenium**     | 50 | 206.45 | 4.13 | *3.13.7 Documentation* | — |\n",
    "|      | **SupaCrawler** | 50 | 41.33  | 0.83 | `# Python 3.13.7 documentation` … | ⚡ **5.0× faster** |\n",
    "| [Google AI Docs](https://ai.google.dev) | **Selenium**     | 50 | 455.67 | 9.11 | *Gemini Developer API \\| Gemma open models \\| Google AI for Developers* | — |\n",
    "|      | **SupaCrawler** | 50 | 37.25  | 0.74 | `[New Gemini 2.5 Flash Image …]` | ⚡ **12.2× faster** |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
