{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e00f8e7c",
   "metadata": {},
   "source": [
    "\n",
    "# Fair BeautifulSoup vs Supacrawler: Complete Performance Comparison\n",
    "\n",
    "This notebook compares BeautifulSoup (with requests) against Supacrawler with similar retry logic that matches Supacrawler's exact implementation.\n",
    "\n",
    "1. **Identical Retry Logic**: Uses the same exponential backoff (1s, 2s, 4s) and retry conditions as Supacrawler\n",
    "2. **Smart Error Classification**: Only retries on retryable errors (429, 503, timeouts) - not 403/404\n",
    "3. **Same Max Retries**: 3 attempts total, matching Supacrawler service\n",
    "4. **Timeout Consistency**: Uses similar timeout patterns to Supacrawler's HTTP implementation\n",
    "\n",
    "Note: BeautifulSoup + requests is similar to Supacrawler's simple HTTP scraping (no JavaScript), so this is a fair comparison for non-JS content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27dd76a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from supacrawler import SupacrawlerClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3f2a66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_retryable_error(error):\n",
    "    \"\"\"\n",
    "    Check if error should be retried - EXACTLY matching Supacrawler's retry logic.\n",
    "    This mirrors the logic in supacrawler/internal/core/scrape/service.go:isRetryableScrapingError\n",
    "    \"\"\"\n",
    "    if error is None:\n",
    "        return False\n",
    "    \n",
    "    error_str = str(error).lower()\n",
    "    \n",
    "    # Rate limiting and server errors (retryable)\n",
    "    if any(term in error_str for term in [\"429\", \"too many requests\", \"rate limit\"]):\n",
    "        return True\n",
    "    if any(term in error_str for term in [\"503\", \"service unavailable\", \"502\", \"bad gateway\", \"504\", \"gateway timeout\"]):\n",
    "        return True\n",
    "    if any(term in error_str for term in [\"connection reset\", \"connection refused\", \"timeout\"]) and \"permanent\" not in error_str:\n",
    "        return True\n",
    "    \n",
    "    # Client errors (NOT retryable - this is crucial for fairness)\n",
    "    if any(term in error_str for term in [\"403\", \"forbidden\", \"404\", \"not found\"]):\n",
    "        return False\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23808dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beautifulsoup_scrape_single(url, max_retries=3):\n",
    "    \"\"\"Single page scraping with BeautifulSoup + requests - FAIR version\"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'\n",
    "    })\n",
    "    \n",
    "    # EXACT Supacrawler retry logic\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Apply backoff BEFORE attempt (if retry)\n",
    "            if attempt > 0:\n",
    "                # EXACT Supacrawler formula: d := time.Duration(1<<(attempt-1)) * time.Second\n",
    "                backoff = 1 << (attempt - 1)  # 1s, 2s, 4s\n",
    "                print(f\"Retry {attempt} for {url} after {backoff}s\")\n",
    "                time.sleep(backoff)\n",
    "            \n",
    "            # Timeout matching Supacrawler's 10s HTTP timeout\n",
    "            response = session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            title = soup.title.string if soup.title else 'No title'\n",
    "            \n",
    "            # Extract text content (similar to body.textContent)\n",
    "            content = soup.get_text()[:200] + \"...\" if len(soup.get_text()) > 200 else soup.get_text()\n",
    "            \n",
    "            return {\n",
    "                'title': title.strip(),\n",
    "                'content': content.strip(),\n",
    "                'time': time.time() - start,\n",
    "                'javascript_support': False,  # BeautifulSoup doesn't execute JS\n",
    "                'resource_usage': 'Low (HTTP only)'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            # CRUCIAL: Use Supacrawler's EXACT retry logic\n",
    "            if is_retryable_error(e) and attempt < max_retries - 1:\n",
    "                print(f\"Retryable error for {url}: {e}\")\n",
    "                continue  # Will apply backoff on next iteration\n",
    "            else:\n",
    "                # Non-retryable error or max retries reached\n",
    "                if attempt >= max_retries - 1:\n",
    "                    print(f\"Failed to scrape {url} after {max_retries} attempts: {e}\")\n",
    "                else:\n",
    "                    print(f\"Non-retryable error for {url}: {e}\")\n",
    "                return {\n",
    "                    'title': 'Error',\n",
    "                    'content': f\"Error: {e}\",\n",
    "                    'time': time.time() - start,\n",
    "                    'javascript_support': False,\n",
    "                    'resource_usage': 'Low (HTTP only)'\n",
    "                }\n",
    "    \n",
    "def supacrawler_scrape_single(url):\n",
    "    \"\"\"Single page scraping with Supacrawler (non-JS for fair comparison)\"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    client = SupacrawlerClient(api_key='')\n",
    "    # Use render_js=False for fair comparison with BeautifulSoup\n",
    "    response = client.scrape(url, format='markdown', render_js=False, fresh=True)\n",
    "    \n",
    "    title = response.metadata.title if response.metadata else 'No title'\n",
    "    content = response.content if response.content else \"No content\"\n",
    "    \n",
    "    return {\n",
    "        'title': title,\n",
    "        'content': content[:200] + \"...\" if len(content) > 200 else content,\n",
    "        'metadata': response.metadata,\n",
    "        'time': time.time() - start,\n",
    "        'javascript_support': False,  # Fair comparison\n",
    "        'resource_usage': 'Zero local resources'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cc7cfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Supabase | The Postgres Development Platform.', 'content': 'Supabase | The Postgres Development Platform.Product Developers Solutions PricingDocsBlog88.5KSign inStart your projectOpen main menuBuild in a weekendScale to millionsSupabase is the Postgres develop...', 'time': 0.3613901138305664, 'javascript_support': False, 'resource_usage': 'Low (HTTP only)'}\n",
      "{'title': 'Supabase | The Postgres Development Platform.', 'content': '# Build in a weekendScale to millions\\nSupabase is the Postgres development platform.\\nStart your project with a Postgres database, Authentication, instant APIs, Edge Functions, Realtime subscriptions, ...', 'metadata': <supacrawler.types.PageMetadata object at 0x10daffe50>, 'time': 0.1987929344177246, 'javascript_support': False, 'resource_usage': 'Zero local resources'}\n"
     ]
    }
   ],
   "source": [
    "url = 'https://supabase.com'\n",
    "\n",
    "bs_output = beautifulsoup_scrape_single(url)\n",
    "print(bs_output)\n",
    "\n",
    "sc_output = supacrawler_scrape_single(url)\n",
    "print(sc_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f9ccd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time difference: -0.1625971794128418 seconds\n",
      "Supacrawler is 1.8179223265107378 times faster than BeautifulSoup\n"
     ]
    }
   ],
   "source": [
    "# time difference\n",
    "print(f\"Time difference: {sc_output['time'] - bs_output['time']} seconds\")\n",
    "print(f\"Supacrawler is {1 / (sc_output['time'] / bs_output['time'])} times faster than BeautifulSoup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1120770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beautifulsoup Content: \n",
      "Supabase | The Postgres Development Platform.Product Developers Solutions PricingDocsBlog88.3KSign inStart your projectOpen main menuBuild in a weekendScale to millionsSupabase is the Postgres develop...\n",
      "\n",
      "Supacrawler Content: \n",
      "# Build in a weekendScale to millions\n",
      "Supabase is the Postgres development platform.\n",
      "Start your project with a Postgres database, Authentication, instant APIs, Edge Functions, Realtime subscriptions, ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# content length\n",
    "print(f\"Beautifulsoup Content: \\n{(bs_output['content'])}\\n\")\n",
    "\n",
    "print(f\"Supacrawler Content: \\n{(sc_output['content'])}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac65ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time, requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def beautifulsoup_crawl(start_url, max_pages=5, max_retries=3):\n",
    "    \"\"\"Multi-page crawling with BeautifulSoup + requests - FAIR version\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'\n",
    "    })\n",
    "    \n",
    "    visited = set()\n",
    "    to_visit = deque([start_url])\n",
    "    results = []\n",
    "    failures = []  # track but exclude from metrics\n",
    "    \n",
    "    while to_visit and len(results) < max_pages:\n",
    "        url = to_visit.popleft()\n",
    "        \n",
    "        if url in visited:\n",
    "            continue\n",
    "        visited.add(url)\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                if attempt > 0:\n",
    "                    backoff = 1 << (attempt - 1)\n",
    "                    time.sleep(backoff)\n",
    "                \n",
    "                response = session.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                title = soup.title.string.strip() if soup.title else \"No title\"\n",
    "                text = soup.get_text()\n",
    "                content = text[:200] + \"...\" if len(text) > 200 else text\n",
    "                \n",
    "                base_domain = urlparse(start_url).netloc\n",
    "                links = []\n",
    "                for link in soup.find_all(\"a\", href=True):\n",
    "                    href = urljoin(url, link[\"href\"])\n",
    "                    if urlparse(href).netloc == base_domain:\n",
    "                        if href not in visited and len(to_visit) < 20:\n",
    "                            links.append(href)\n",
    "                            to_visit.append(href)\n",
    "                \n",
    "                results.append({\n",
    "                    \"url\": url,\n",
    "                    \"title\": title,\n",
    "                    \"content\": content.strip(),\n",
    "                    \"links_found\": len(links),\n",
    "                    \"metadata\": {\n",
    "                        \"status_code\": response.status_code,\n",
    "                        \"word_count\": len(text.split()),\n",
    "                        \"headers\": [h.get_text(strip=True) for h in soup.find_all([\"h1\", \"h2\", \"h3\"])],\n",
    "                    }\n",
    "                })\n",
    "                \n",
    "                break  # success, exit retry loop\n",
    "                \n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    failures.append({\n",
    "                        \"url\": url,\n",
    "                        \"error\": str(e)\n",
    "                    })\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    return {\n",
    "        \"pages_crawled\": len(results),   # only successes\n",
    "        \"failures\": failures,            # track errors separately\n",
    "        \"total_time\": end_time - start_time,\n",
    "        \"avg_time_per_page\": (end_time - start_time) / len(results) if results else 0,\n",
    "        \"results\": results\n",
    "    }\n",
    "\n",
    "\n",
    "def supacrawler_crawl(start_url, max_pages=5):\n",
    "    \"\"\"Built-in crawling with SupaCrawler (SDK-native usage)\"\"\"\n",
    "    start_time = time.time()\n",
    "    client = SupacrawlerClient(api_key=\"\")\n",
    "\n",
    "    try:\n",
    "        job = client.create_crawl_job(\n",
    "            url=start_url,\n",
    "            format=\"markdown\",\n",
    "            link_limit=max_pages,\n",
    "            depth=3,\n",
    "            include_subdomains=False,\n",
    "            render_js=False,\n",
    "            fresh=True # fresh never uses cached results\n",
    "        )\n",
    "\n",
    "        crawl_output = client.wait_for_crawl(\n",
    "            job.job_id,\n",
    "            interval_seconds=1.0,\n",
    "            timeout_seconds=120.0\n",
    "        )\n",
    "\n",
    "        crawl_data = crawl_output.data.crawl_data\n",
    "        end_time = time.time()\n",
    "\n",
    "        return {\n",
    "            \"pages_crawled\": len(crawl_data),\n",
    "            \"total_time\": end_time - start_time,\n",
    "            \"avg_time_per_page\": (end_time - start_time) / len(crawl_data) if crawl_data else 0,\n",
    "            \"crawl_data\": crawl_data  # keep the native objects\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        end_time = time.time()\n",
    "        return {\n",
    "            \"pages_crawled\": 0,\n",
    "            \"total_time\": end_time - start_time,\n",
    "            \"error\": str(e)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbc2255d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Site Crawling Comparison\n",
      "\n",
      "\n",
      "============================================================\n",
      "Benchmarking: https://nodejs.org/docs\n",
      "Max pages: 50\n",
      "============================================================\n",
      "\n",
      "[BeautifulSoup] Manual crawling:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Pages crawled: 50\n",
      "  Total time: 109.19s\n",
      "  Avg per page: 2.18s\n",
      "  First page title: Index of /docs/\n",
      "  Metadata: {'status_code': 200, 'word_count': 3454, 'headers': ['Index of /docs/']}\n",
      "\n",
      "[SupaCrawler] Built-in crawling:\n",
      "  Pages crawled: 50\n",
      "  Total time: 65.51s\n",
      "  Avg per page: 1.31s\n",
      "  First page markdown preview: # Run JavaScript Everywhere\n",
      "Node.js® is a free, open-source, cross-platform JavaScript runtime environment\n",
      "that lets dev ...\n",
      "  Metadata: {'title': 'Node.js — Run JavaScript Everywhere', 'status_code': 200, 'description': 'Node.js® is a free, open-source, cross-platform JavaScript runtime environment that lets developers create servers, web apps, command line tools and scripts.', 'canonical': 'https://nodejs.org/en', 'favicon': 'https://nodejs.org/static/images/favicons/favicon.png', 'og_title': 'Node.js — Run JavaScript Everywhere', 'og_description': 'Node.js® is a free, open-source, cross-platform JavaScript runtime environment that lets developers create servers, web apps, command line tools and scripts.', 'og_image': 'https://nodejs.org/en/next-data/og/announcement/Node.js%20%E2%80%94%20Run%20JavaScript%20Everywhere', 'twitter_title': 'Node.js — Run JavaScript Everywhere', 'twitter_description': 'Node.js® is a free, open-source, cross-platform JavaScript runtime environment that lets developers create servers, web apps, command line tools and scripts.', 'twitter_image': 'https://nodejs.org/static/images/logo-hexagon-card.png', 'source_url': 'https://nodejs.org'}\n",
      "\n",
      "⚡ Performance: SupaCrawler is 1.7x faster per page\n",
      "\n",
      "============================================================\n",
      "Benchmarking: https://docs.python.org\n",
      "Max pages: 50\n",
      "============================================================\n",
      "\n",
      "[BeautifulSoup] Manual crawling:\n",
      "  Pages crawled: 50\n",
      "  Total time: 3.61s\n",
      "  Avg per page: 0.07s\n",
      "  First page title: 3.13.7 Documentation\n",
      "  Metadata: {'status_code': 200, 'word_count': 415, 'headers': ['Download', 'Docs by version', 'Other resources', 'Navigation', 'Python 3.13.7 documentation', 'Download', 'Docs by version', 'Other resources', 'Navigation']}\n",
      "\n",
      "[SupaCrawler] Built-in crawling:\n",
      "  Pages crawled: 50\n",
      "  Total time: 7.09s\n",
      "  Avg per page: 0.14s\n",
      "  First page markdown preview: # Python 3.13.7 documentation\n",
      "Welcome! This is the official documentation for Python 3.13.7.\n",
      "**Documentation sections:** ...\n",
      "  Metadata: {'title': '3.13.7 Documentation', 'status_code': 200, 'description': 'The official Python documentation.', 'canonical': 'https://docs.python.org/3/index.html', 'favicon': 'https://docs.python.org/_static/py.svg', 'og_title': 'Python 3.13 documentation', 'og_description': 'The official Python documentation.', 'og_image': 'https://docs.python.org/3/_static/og-image.png', 'source_url': 'https://docs.python.org'}\n",
      "\n",
      "⚡ Performance: SupaCrawler is 0.5x faster per page\n",
      "\n",
      "============================================================\n",
      "Benchmarking: https://go.dev/doc/\n",
      "Max pages: 50\n",
      "============================================================\n",
      "\n",
      "[BeautifulSoup] Manual crawling:\n",
      "  Pages crawled: 50\n",
      "  Total time: 25.10s\n",
      "  Avg per page: 0.50s\n",
      "  First page title: Documentation - The Go Programming Language\n",
      "  Metadata: {'status_code': 200, 'word_count': 1929, 'headers': ['Documentation', 'Getting Started', 'Installing Go', 'Tutorial: Getting started', 'Tutorial: Create a module', 'Tutorial: Getting started with multi-module workspaces', 'Tutorial: Developing a RESTful API with Go and Gin', 'Tutorial: Getting started with generics', 'Tutorial: Getting started with fuzzing', 'Writing Web Applications', 'How to write Go code', 'A Tour of Go', 'Using and understanding Go', 'Effective Go', 'Frequently Asked Questions (FAQ)', 'Editor plugins and IDEs', 'Diagnostics', 'A Guide to the Go Garbage Collector', 'Managing dependencies', 'Fuzzing', 'Coverage for Go applications', 'Profile-guided optimization', 'References', 'Package Documentation', 'Command Documentation', 'Language Specification', 'Go Modules Reference', 'go.mod file reference', 'The Go Memory Model', 'Contribution Guide', 'Release History', 'Accessing databases', 'Tutorial: Accessing a relational database', 'Accessing relational databases', 'Opening a database handle', \"Executing SQL statements that don't return data\", 'Querying for data', 'Using prepared statements', 'Executing transactions', 'Canceling in-progress database operations', 'Managing connections', 'Avoiding SQL injection risk', 'Developing modules', 'Developing and publishing modules', 'Module release and versioning workflow', 'Managing module source', 'Organizing a Go module', 'Developing a major version update', 'Publishing a module', 'Module version numbering', 'Talks', 'A Video Tour of Go', 'Code that grows with grace', 'Go Concurrency Patterns', 'Advanced Go Concurrency Patterns', 'Codewalks', 'Language', 'Packages', 'Modules', 'Tools', 'Wiki', 'Non-English Documentation']}\n",
      "\n",
      "[SupaCrawler] Built-in crawling:\n",
      "  Pages crawled: 50\n",
      "  Total time: 17.16s\n",
      "  Avg per page: 0.34s\n",
      "  First page markdown preview: # Build simple, secure, scalable systems with Go\n",
      "-\n",
      "An open-source programming language supported by Google\n",
      "-\n",
      "Easy to lea ...\n",
      "  Metadata: {'title': 'The Go Programming Language', 'status_code': 200, 'description': 'Go is an open source programming language that makes it simple to build secure, scalable systems.', 'favicon': 'https://go.dev/images/favicon-gopher.png', 'og_title': 'The Go Programming Language', 'og_description': 'Go is an open source programming language that makes it simple to build secure, scalable systems.', 'og_image': 'https://go.dev/doc/gopher/gopher5logo.jpg', 'twitter_image': 'https://go.dev/doc/gopher/gopherbelly300.jpg', 'source_url': 'https://go.dev'}\n",
      "\n",
      "⚡ Performance: SupaCrawler is 1.5x faster per page\n"
     ]
    }
   ],
   "source": [
    "# Benchmark Runner: Compare BeautifulSoup vs SupaCrawler across multiple sites\n",
    "test_sites = [\n",
    "    \"https://nodejs.org/docs\",\n",
    "    \"https://docs.python.org\",\n",
    "    \"https://go.dev/doc/\",\n",
    "]\n",
    "max_pages = 50\n",
    "\n",
    "def run_comparison(crawl_url: str, max_pages: int):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"Benchmarking: {crawl_url}\")\n",
    "    print(f\"Max pages: {max_pages}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # --- BeautifulSoup ---\n",
    "    print(\"\\n[BeautifulSoup] Manual crawling:\")\n",
    "    bs_result = beautifulsoup_crawl(crawl_url, max_pages)\n",
    "    print(f\"  Pages crawled: {bs_result['pages_crawled']}\")\n",
    "    print(f\"  Total time: {bs_result['total_time']:.2f}s\")\n",
    "    print(f\"  Avg per page: {bs_result['avg_time_per_page']:.2f}s\")\n",
    "\n",
    "    if bs_result[\"pages_crawled\"] > 0:\n",
    "        first_page = bs_result[\"results\"][0]\n",
    "        print(\"  First page title:\", first_page[\"title\"])\n",
    "        print(\"  Metadata:\", first_page[\"metadata\"])\n",
    "\n",
    "    # --- SupaCrawler ---\n",
    "    print(\"\\n[SupaCrawler] Built-in crawling:\")\n",
    "    sc_result = supacrawler_crawl(crawl_url, max_pages)\n",
    "\n",
    "    if \"error\" in sc_result:\n",
    "        print(f\"  Error: {sc_result['error']}\")\n",
    "    else:\n",
    "        print(f\"  Pages crawled: {sc_result['pages_crawled']}\")\n",
    "        print(f\"  Total time: {sc_result['total_time']:.2f}s\")\n",
    "        print(f\"  Avg per page: {sc_result['avg_time_per_page']:.2f}s\")\n",
    "\n",
    "        if sc_result[\"pages_crawled\"] > 0:\n",
    "            first_url = sc_result[\"crawl_data\"][0]\n",
    "            print(\"  First page markdown preview:\", first_url.markdown[:120], \"...\")\n",
    "            print(\"  Metadata:\", first_url.metadata.to_json())\n",
    "\n",
    "    # --- Performance summary ---\n",
    "    if (\n",
    "        bs_result[\"pages_crawled\"] > 0\n",
    "        and sc_result.get(\"pages_crawled\", 0) > 0\n",
    "        and \"error\" not in sc_result\n",
    "    ):\n",
    "        ratio = bs_result[\"avg_time_per_page\"] / sc_result[\"avg_time_per_page\"]\n",
    "        print(f\"\\n⚡ Performance: SupaCrawler is {ratio:.1f}x faster per page\")\n",
    "\n",
    "# --- Run benchmark for all test sites ---\n",
    "print(\"Multi-Site Crawling Comparison\\n\")\n",
    "for site in test_sites:\n",
    "    run_comparison(site, max_pages)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
