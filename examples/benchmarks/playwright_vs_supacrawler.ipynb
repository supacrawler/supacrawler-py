{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playwright vs SupaCrawler: Complete Performance Comparison\n",
    "\n",
    "This notebook compares Playwright (Python) against SupaCrawler for both single-page scraping and multi-page crawling. We focus on performance, infrastructure requirements, and development complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation requirements\n",
    "# !pip install playwright\n",
    "# !playwright install chromium\n",
    "# !pip install supacrawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from playwright.async_api import async_playwright\n",
    "from supacrawler import SupacrawlerClient\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Single Page Scraping Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Page Scraping Comparison\n",
      "===================================\n",
      "Test URL: https://example.com\n",
      "\n",
      "Playwright:\n",
      "Title: Example Domain\n",
      "Time: 7.58s\n",
      "\n",
      "SupaCrawler:\n",
      "Title: Example Domain\n",
      "Time: 1.21s\n",
      "\n",
      "Performance: SupaCrawler is 6.3x faster\n",
      "Setup complexity: Playwright (browser install + 1GB), SupaCrawler (API key only)\n"
     ]
    }
   ],
   "source": [
    "async def playwright_scrape(url):\n",
    "    \"\"\"Single page scraping with Playwright\"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        context = await browser.new_context(\n",
    "            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        )\n",
    "        page = await context.new_page()\n",
    "        \n",
    "        try:\n",
    "            await page.goto(url, wait_until='networkidle')\n",
    "            \n",
    "            title = await page.title()\n",
    "            content = await page.evaluate('document.body.textContent')\n",
    "            \n",
    "            result = {\n",
    "                'title': title,\n",
    "                'content': content[:200] + \"...\" if len(content) > 200 else content,\n",
    "                'time': time.time() - start,\n",
    "                'javascript_support': True,\n",
    "            }\n",
    "            \n",
    "        finally:\n",
    "            await browser.close()\n",
    "    \n",
    "    return result\n",
    "\n",
    "def supacrawler_scrape(url):\n",
    "    \"\"\"Single page scraping with SupaCrawler\"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    client = SupacrawlerClient(api_key='')\n",
    "    response = client.scrape(url, format='markdown', render_js=True, fresh=True)\n",
    "    \n",
    "    title = response.metadata.title if response.metadata else 'No title'\n",
    "    content = response.markdown if response.markdown else \"No content\"\n",
    "    \n",
    "    return {\n",
    "        'title': title,\n",
    "        'content': content[:200] + \"...\" if len(content) > 200 else content,\n",
    "        'time': time.time() - start,\n",
    "        'javascript_support': True,\n",
    "    }\n",
    "\n",
    "# Test single page scraping\n",
    "async def run_scraping_test():\n",
    "    test_url = 'https://example.com'\n",
    "    \n",
    "    print(\"Single Page Scraping Comparison\")\n",
    "    print(\"=\" * 35)\n",
    "    print(f\"Test URL: {test_url}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Playwright:\")\n",
    "    playwright_result = await playwright_scrape(test_url)\n",
    "    print(f\"Title: {playwright_result['title']}\")\n",
    "    print(f\"Time: {playwright_result['time']:.2f}s\")\n",
    "    print()\n",
    "    \n",
    "    print(\"SupaCrawler:\")\n",
    "    sc_result = supacrawler_scrape(test_url)\n",
    "    print(f\"Title: {sc_result['title']}\")\n",
    "    print(f\"Time: {sc_result['time']:.2f}s\")\n",
    "    print()\n",
    "    \n",
    "    if playwright_result['time'] > 0 and sc_result['time'] > 0:\n",
    "        ratio = playwright_result['time'] / sc_result['time']\n",
    "        print(f\"Performance: SupaCrawler is {ratio:.1f}x faster\")\n",
    "    \n",
    "    print(f\"Setup complexity: Playwright (browser install + 1GB), SupaCrawler (API key only)\")\n",
    "\n",
    "# Run the async function\n",
    "await run_scraping_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Multi-Page Crawling Comparison\n",
    "\n",
    "### How SupaCrawler Crawl Works\n",
    "\n",
    "1. **Crawl dispatch** → Crawl breaks the target domain into a queue of page URLs.\n",
    "2. **Per-page scrape** → For each URL, crawl calls the exact same **scrape service** that is exposed publicly (`/scrape`).\n",
    "3. **Retry & backoff** → Each scrape runs with up to **3 retries** and **exponential backoff** (1s → 2s → 4s).\n",
    "4. **Error handling** → If all retries fail, the page is marked as failed.\n",
    "5. **Aggregation** → Successfully scraped pages are aggregated into the crawl result.\n",
    "\n",
    "To make the playwright and Playwright benchmarks fair, we applied the **same retry + backoff algorithm** when fetching each page.\n",
    "\n",
    "The crawl path is literally just a loop of our scrape service. That means the **retry, backoff, and error logic** are identical between crawl and scrape. To make the playwright and Playwright benchmarks fair, we applied the **same retry + backoff algorithm** when fetching each page.\n",
    "\n",
    "Therefore, **all three systems (SupaCrawler, playwright, Playwright)** are operating under identical conditions — same error tolerance, same retry count, same backoff schedule.\n",
    "\n",
    "This guarantees the benchmark is **apples-to-apples**: differences come from **architecture and performance**, not from error-handling or retry policies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import random\n",
    "import time\n",
    "from collections import deque\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "async def playwright_crawl(start_url, max_pages=5, max_retries=3):\n",
    "    \"\"\"Manual crawling with Playwright + retries/backoff\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    visited = set()\n",
    "    to_visit = deque([start_url])\n",
    "    results = []\n",
    "    \n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        context = await browser.new_context(\n",
    "            user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            while to_visit and len(results) < max_pages:\n",
    "                url = to_visit.popleft()\n",
    "                if url in visited:\n",
    "                    continue\n",
    "                visited.add(url)\n",
    "\n",
    "                attempt = 0\n",
    "                success = False\n",
    "\n",
    "                while attempt < max_retries and not success:\n",
    "                    try:\n",
    "                        page = await context.new_page()\n",
    "                        await page.goto(url, wait_until=\"networkidle\", timeout=30000)\n",
    "\n",
    "                        # Extract title and content\n",
    "                        title = await page.title() or \"N/A\"\n",
    "                        body_content = await page.evaluate(\"document.body.textContent\")\n",
    "                        content = body_content[:500] + \"...\" if len(body_content) > 500 else body_content\n",
    "\n",
    "                        # Meta tags\n",
    "                        try:\n",
    "                            description = await page.locator(\"meta[name=description]\").get_attribute(\"content\")\n",
    "                        except:\n",
    "                            description = None\n",
    "                        try:\n",
    "                            keywords = await page.locator(\"meta[name=keywords]\").get_attribute(\"content\")\n",
    "                        except:\n",
    "                            keywords = None\n",
    "\n",
    "                        # Headers\n",
    "                        headers = []\n",
    "                        for tag in [\"h1\", \"h2\", \"h3\"]:\n",
    "                            elems = await page.locator(tag).all()\n",
    "                            for elem in elems:\n",
    "                                text = (await elem.inner_text()).strip()\n",
    "                                if text:\n",
    "                                    headers.append({tag: text})\n",
    "\n",
    "                        # Links (same domain only)\n",
    "                        base_domain = urlparse(start_url).netloc\n",
    "                        links = []\n",
    "                        link_elements = await page.locator(\"a[href]\").all()\n",
    "                        for link in link_elements:\n",
    "                            href = await link.get_attribute(\"href\")\n",
    "                            if href:\n",
    "                                full_url = urljoin(url, href)\n",
    "                                if urlparse(full_url).netloc == base_domain:\n",
    "                                    links.append(full_url)\n",
    "                                    if full_url not in visited and len(to_visit) < 20:\n",
    "                                        to_visit.append(full_url)\n",
    "\n",
    "                        # Metadata\n",
    "                        metadata = {\n",
    "                            \"url\": url,\n",
    "                            \"title\": title,\n",
    "                            \"description\": description,\n",
    "                            \"keywords\": keywords,\n",
    "                            \"headers\": headers,\n",
    "                            \"word_count\": len(body_content.split()),\n",
    "                            \"links_found\": len(links),\n",
    "                        }\n",
    "\n",
    "                        results.append({\n",
    "                            \"content\": content,\n",
    "                            \"metadata\": metadata\n",
    "                        })\n",
    "\n",
    "                        success = True\n",
    "                        await page.close()\n",
    "                        await asyncio.sleep(random.uniform(1, 2))  # avoid fixed patterns\n",
    "\n",
    "                    except Exception as e:\n",
    "                        attempt += 1\n",
    "                        if attempt < max_retries:\n",
    "                            backoff = 2 ** (attempt - 1)  # exponential backoff\n",
    "                            print(f\"Retry {attempt} for {url} after {backoff}s due to {e}\")\n",
    "                            await asyncio.sleep(backoff)\n",
    "                        else:\n",
    "                            print(f\"Failed to scrape {url} after {max_retries} attempts\")\n",
    "                            break\n",
    "\n",
    "        finally:\n",
    "            await browser.close()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return {\n",
    "        \"pages_crawled\": len(results),\n",
    "        \"total_time\": end_time - start_time,\n",
    "        \"avg_time_per_page\": (end_time - start_time) / len(results) if results else 0,\n",
    "        \"results\": results\n",
    "    }\n",
    "\n",
    "\n",
    "def supacrawler_crawl(start_url, max_pages=5):\n",
    "    \"\"\"Built-in crawling with SupaCrawler (SDK-native usage)\"\"\"\n",
    "    start_time = time.time()\n",
    "    client = SupacrawlerClient(api_key=\"\")\n",
    "\n",
    "    try:\n",
    "        job = client.create_crawl_job(\n",
    "            url=start_url,\n",
    "            format=\"markdown\",\n",
    "            link_limit=max_pages,\n",
    "            depth=3,\n",
    "            include_subdomains=False,\n",
    "            render_js=True,\n",
    "            fresh=True # fresh never uses cached results\n",
    "        )\n",
    "\n",
    "        crawl_output = client.wait_for_crawl(\n",
    "            job.job_id,\n",
    "            interval_seconds=1.0,\n",
    "            timeout_seconds=120.0\n",
    "        )\n",
    "\n",
    "        crawl_data = crawl_output.data.crawl_data\n",
    "        end_time = time.time()\n",
    "\n",
    "        return {\n",
    "            \"pages_crawled\": len(crawl_data),\n",
    "            \"total_time\": end_time - start_time,\n",
    "            \"avg_time_per_page\": (end_time - start_time) / len(crawl_data) if crawl_data else 0,\n",
    "            \"crawl_data\": crawl_data  # keep the native objects\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        end_time = time.time()\n",
    "        return {\n",
    "            \"pages_crawled\": 0,\n",
    "            \"total_time\": end_time - start_time,\n",
    "            \"error\": str(e)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Page Crawling Comparison\n",
      "===================================\n",
      "Test URL: https://docs.python.org\n",
      "Max pages: 5\n",
      "\n",
      "Playwright manual crawling:\n",
      "Pages crawled: 5\n",
      "Total time: 164.39s\n",
      "Average per page: 32.88s\n",
      "\n",
      "SupaCrawler built-in crawling:\n",
      "Pages crawled: 5\n",
      "Total time: 5.08s\n",
      "Average per page: 1.02s\n",
      "Performance: SupaCrawler is 32.4x faster per page\n",
      "Playwright success rate: 100.0%\n",
      "SupaCrawler success rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Test crawling\n",
    "async def run_crawling_test():\n",
    "    crawl_url = \"https://docs.python.org\"\n",
    "    max_pages = 5\n",
    "    \n",
    "    print(\"Multi-Page Crawling Comparison\")\n",
    "    print(\"=\" * 35)\n",
    "    print(f\"Test URL: {crawl_url}\")\n",
    "    print(f\"Max pages: {max_pages}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Playwright manual crawling:\")\n",
    "    playwright_crawl_result = await playwright_crawl(crawl_url, max_pages)\n",
    "    print(f\"Pages crawled: {playwright_crawl_result['pages_crawled']}\")\n",
    "    print(f\"Total time: {playwright_crawl_result['total_time']:.2f}s\")\n",
    "    print(f\"Average per page: {playwright_crawl_result['avg_time_per_page']:.2f}s\\n\")\n",
    "    \n",
    "    print(\"SupaCrawler built-in crawling:\")\n",
    "    sc_crawl_result = supacrawler_crawl(crawl_url, max_pages)\n",
    "    \n",
    "    if 'error' in sc_crawl_result:\n",
    "        print(f\"Error: {sc_crawl_result['error']}\")\n",
    "    else:\n",
    "        print(f\"Pages crawled: {sc_crawl_result['pages_crawled']}\")\n",
    "        print(f\"Total time: {sc_crawl_result['total_time']:.2f}s\")\n",
    "        print(f\"Average per page: {sc_crawl_result['avg_time_per_page']:.2f}s\")\n",
    "        \n",
    "    \n",
    "    # Performance summary\n",
    "    if (playwright_crawl_result['pages_crawled'] > 0 and sc_crawl_result.get('pages_crawled', 0) > 0 and 'error' not in sc_crawl_result):\n",
    "        speed_ratio = playwright_crawl_result['avg_time_per_page'] / sc_crawl_result['avg_time_per_page']\n",
    "        print(f\"Performance: SupaCrawler is {speed_ratio:.1f}x faster per page\")\n",
    "    \n",
    "    print(f\"Playwright success rate: {playwright_crawl_result['pages_crawled'] / max_pages * 100:.1f}%\")\n",
    "    print(f\"SupaCrawler success rate: {sc_crawl_result.get('pages_crawled', 0) / max_pages * 100:.1f}%\")\n",
    "\n",
    "# Run the async function\n",
    "await run_crawling_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: A more comprehensive test for crawling multiple pages!\n",
    "Let's try running for 50 pages on 3 different websites this time:\n",
    "-  supacrawler.com\n",
    "-  supabase.com\n",
    "-  ai.google.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_comparison(crawl_url: str, max_pages: int):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"Benchmarking: {crawl_url}\")\n",
    "    print(f\"Max pages: {max_pages}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # --- Playwright ---\n",
    "    print(\"\\n[Playwright] Manual crawling:\")\n",
    "    playwright_result = await playwright_crawl(crawl_url, max_pages)\n",
    "    print(f\"  Pages crawled: {playwright_result['pages_crawled']}\")\n",
    "    print(f\"  Total time: {playwright_result['total_time']:.2f}s\")\n",
    "    print(f\"  Avg per page: {playwright_result['avg_time_per_page']:.2f}s\")\n",
    "\n",
    "    if playwright_result[\"pages_crawled\"] > 0:\n",
    "        first_page = playwright_result[\"results\"][0]\n",
    "        print(\"  First page title:\", first_page[\"metadata\"][\"title\"])\n",
    "\n",
    "    # --- SupaCrawler ---\n",
    "    print(\"\\n[SupaCrawler] Built-in crawling:\")\n",
    "    sc_result = supacrawler_crawl(crawl_url, max_pages)\n",
    "\n",
    "    if \"error\" in sc_result:\n",
    "        print(f\"  Error: {sc_result['error']}\")\n",
    "    else:\n",
    "        print(f\"  Pages crawled: {sc_result['pages_crawled']}\")\n",
    "        print(f\"  Total time: {sc_result['total_time']:.2f}s\")\n",
    "        print(f\"  Avg per page: {sc_result['avg_time_per_page']:.2f}s\")\n",
    "\n",
    "        if sc_result[\"pages_crawled\"] > 0:\n",
    "            first_url = sc_result[\"crawl_data\"][0]\n",
    "            print(\"  First page markdown preview:\", first_url.markdown[:120], \"...\")\n",
    "\n",
    "    # --- Performance summary ---\n",
    "    if (\n",
    "        playwright_result[\"pages_crawled\"] > 0\n",
    "        and sc_result.get(\"pages_crawled\", 0) > 0\n",
    "        and \"error\" not in sc_result\n",
    "    ):\n",
    "        ratio = playwright_result[\"avg_time_per_page\"] / sc_result[\"avg_time_per_page\"]\n",
    "        print(f\"\\n⚡ Performance: SupaCrawler is {ratio:.1f}x faster per page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Benchmarking: https://supabase.com\n",
      "Max pages: 50\n",
      "============================================================\n",
      "\n",
      "[Playwright] Manual crawling:\n",
      "Retry 1 for https://supabase.com/dashboard after 1s due to Page.goto: Timeout 30000ms exceeded.\n",
      "Call log:\n",
      "  - navigating to \"https://supabase.com/dashboard\", waiting until \"networkidle\"\n",
      "\n",
      "Retry 2 for https://supabase.com/dashboard after 2s due to Page.goto: Timeout 30000ms exceeded.\n",
      "Call log:\n",
      "  - navigating to \"https://supabase.com/dashboard\", waiting until \"networkidle\"\n",
      "\n",
      "Failed to scrape https://supabase.com/dashboard after 3 attempts\n",
      "Retry 1 for https://supabase.com/dashboard/support/new after 1s due to Page.goto: Timeout 30000ms exceeded.\n",
      "Call log:\n",
      "  - navigating to \"https://supabase.com/dashboard/support/new\", waiting until \"networkidle\"\n",
      "\n",
      "Retry 2 for https://supabase.com/dashboard/support/new after 2s due to Page.goto: Timeout 30000ms exceeded.\n",
      "Call log:\n",
      "  - navigating to \"https://supabase.com/dashboard/support/new\", waiting until \"networkidle\"\n",
      "\n",
      "Failed to scrape https://supabase.com/dashboard/support/new after 3 attempts\n",
      "  Pages crawled: 50\n",
      "  Total time: 1871.31s\n",
      "  Avg per page: 37.43s\n",
      "  First page title: Supabase | The Postgres Development Platform.\n",
      "\n",
      "[SupaCrawler] Built-in crawling:\n",
      "  Pages crawled: 50\n",
      "  Total time: 32.34s\n",
      "  Avg per page: 0.65s\n",
      "  First page markdown preview: # Build in a weekendScale to millions\n",
      "Supabase is the Postgres development platform.\n",
      "Start your project with a Postgres  ...\n",
      "\n",
      "⚡ Performance: SupaCrawler is 57.9x faster per page\n"
     ]
    }
   ],
   "source": [
    "# Benchmark Runner: Compare Playwright vs SupaCrawler across multiple sites\n",
    "test_sites = [\n",
    "    \"https://supabase.com\",\n",
    "    \"https://docs.python.org\",\n",
    "    \"https://ai.google.dev\",\n",
    "]\n",
    "max_pages = 50\n",
    "\n",
    "await run_comparison(test_sites[0], max_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Benchmarking: https://docs.python.org\n",
      "Max pages: 50\n",
      "============================================================\n",
      "\n",
      "[Playwright] Manual crawling:\n",
      "  Pages crawled: 50\n",
      "  Total time: 2775.49s\n",
      "  Avg per page: 55.51s\n",
      "  First page title: 3.13.7 Documentation\n",
      "\n",
      "[SupaCrawler] Built-in crawling:\n",
      "  Pages crawled: 50\n",
      "  Total time: 35.32s\n",
      "  Avg per page: 0.71s\n",
      "  First page markdown preview: # Python 3.13.7 documentation\n",
      "Welcome! This is the official documentation for Python 3.13.7.\n",
      "**Documentation sections:** ...\n",
      "\n",
      "⚡ Performance: SupaCrawler is 78.6x faster per page\n"
     ]
    }
   ],
   "source": [
    "await run_comparison(test_sites[1], max_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Benchmarking: https://ai.google.dev\n",
      "Max pages: 50\n",
      "============================================================\n",
      "\n",
      "[Playwright] Manual crawling:\n",
      "Retry 1 for https://ai.google.dev/gemini-api/docs/video after 1s due to Page.goto: Timeout 30000ms exceeded.\n",
      "Call log:\n",
      "  - navigating to \"https://ai.google.dev/gemini-api/docs/video\", waiting until \"networkidle\"\n",
      "\n",
      "Retry 2 for https://ai.google.dev/gemini-api/docs/video after 2s due to Page.goto: Timeout 30000ms exceeded.\n",
      "Call log:\n",
      "  - navigating to \"https://ai.google.dev/gemini-api/docs/video\", waiting until \"networkidle\"\n",
      "\n",
      "Failed to scrape https://ai.google.dev/gemini-api/docs/video after 3 attempts\n",
      "  Pages crawled: 50\n",
      "  Total time: 1433.59s\n",
      "  Avg per page: 28.67s\n",
      "  First page title: Gemini Developer API | Gemma open models  |  Google AI for Developers\n",
      "\n",
      "[SupaCrawler] Built-in crawling:\n",
      "  Pages crawled: 50\n",
      "  Total time: 36.44s\n",
      "  Avg per page: 0.73s\n",
      "  First page markdown preview: [NewGemini 2.5 Flash Image (aka Nano Banana) is now available in the Gemini API!](https://ai.google.dev/gemini-api/docs/ ...\n",
      "\n",
      "⚡ Performance: SupaCrawler is 39.3x faster per page\n"
     ]
    }
   ],
   "source": [
    "await run_comparison(test_sites[2], max_pages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
