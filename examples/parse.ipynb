{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Supacrawler Python SDK - Parse Examples\n",
        "\n",
        "This notebook demonstrates how to use the Parse API for intelligent AI-powered data extraction using natural language prompts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parse API client initialized!\n",
            "Using hosted API\n"
          ]
        }
      ],
      "source": [
        "# Setup and imports\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from supacrawler import SupacrawlerClient\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize client - uses local engine if no API key provided\n",
        "SUPACRAWLER_API_KEY = os.environ.get(\"SUPACRAWLER_API_KEY\")\n",
        "client = SupacrawlerClient(api_key=SUPACRAWLER_API_KEY)\n",
        "\n",
        "print(\"Parse API client initialized!\")\n",
        "print(f\"Using {'hosted API' if SUPACRAWLER_API_KEY else 'local engine'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Parse Examples\n",
        "\n",
        "The Parse API understands natural language prompts and automatically decides whether to scrape single pages or crawl multiple pages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response:\n",
            "Job enqueued: True\n",
            "Job ID: 5ecf3b7c-a461-4647-a27b-3c06c48206f2\n"
          ]
        }
      ],
      "source": [
        "# Simple product extraction from a single page\n",
        "response = client.parse(\"\"\"Parse blog articles from https://supacrawler.com/blog and return JSON with:\n",
        "  - Title\n",
        "  - Published date\n",
        "  - Summary (2 sentences)\n",
        "  - Tags\n",
        "  - Reading time\"\"\")\n",
        "\n",
        "print(\"Response:\")\n",
        "print(f\"Job enqueued: {response.success}\")\n",
        "print(f\"Job ID: {response.job_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ParseJob(success=True, job_id='5ecf3b7c-a461-4647-a27b-3c06c48206f2', status='completed', type='parse', results={'parse_result': {'data': [{'published_date': 'August 28, 2025', 'reading_time': '10 minutes', 'summary': 'Retrieval-Augmented Generation (RAG) is a powerful technique that enhances Large Language Models (LLMs) by providing them with up-to-date, external knowledge. This guide provides a complete, end-to-end walkthrough of how to crawl an entire website to build a knowledge base, chunk the crawled content, embed the chunks into vectors, and store and query those vectors in a PostgreSQL database using the `pgvector` extension.', 'tags': 'rag, ai, llm, pgvector, supabase, langchain, llamaindex', 'title': 'Building a Production-Ready RAG Pipeline with Supacrawler and pgvector'}, {'published_date': 'August 26, 2025', 'reading_time': '5 minutes', 'summary': \"Watching websites for changes is a perfect use case for automation. Instead of manually checking or building brittle polling scripts, use Supacrawler's Watch API to schedule automatic checks, detect content changes with precision, and receive email notifications when changes occur.\", 'tags': 'watch api, automation, monitoring, website changes', 'title': 'Automate Monitoring Website Changes Automatically with Watch API'}, {'published_date': 'August 8, 2025', 'reading_time': '5 minutes', 'summary': \"Building a custom crawler can be a multi-week infrastructure project. This guide shows you how to crawl an entire documentation website with a single API call using the Supacrawler Jobs API. Get back a clean JSON object where each page's content is stored as markdown.\", 'tags': 'web crawling, crawl api, documentation, data extraction', 'title': 'Crawling a Full Documentation Site Using the Crawl Endpoint'}, {'published_date': 'September 12, 2025', 'reading_time': '7 minutes', 'summary': 'We benchmarked BeautifulSoup + requests against Supacrawler for static content scraping using identical retry and error handling logic. Supacrawler is purpose-built for LLMs and does significant additional processing: content cleaning, markdown conversion, metadata extraction, and noise removal.', 'tags': 'benchmarks, web scraping, beautifulsoup, performance', 'title': 'Supacrawler vs BeautifulSoup: Local Performance Benchmarks'}, {'published_date': 'September 12, 2025', 'reading_time': '10 minutes', 'summary': \"We benchmarked Playwright against Supacrawler for JavaScript-heavy web scraping using identical retry and error handling logic. Supacrawler uses a Go-based streaming worker pool vs Playwright's Python async sequential processing, leading to dramatic performance differences.\", 'tags': 'benchmarks, web scraping, playwright, performance, python', 'title': 'Supacrawler vs Playwright: Local Python Performance Benchmarks'}, {'published_date': 'September 12, 2025', 'reading_time': '10 minutes', 'summary': \"We benchmarked Selenium WebDriver against Supacrawler for JavaScript-heavy web scraping using identical retry and error handling logic. Supacrawler uses a Go-based streaming worker pool architecture vs Selenium's Python sequential processing, resulting in a significant performance advantage.\", 'tags': 'benchmarks, web scraping, selenium, performance, python', 'title': 'Supacrawler vs Selenium: Local Python Performance Benchmarks'}, {'published_date': 'September 11, 2025', 'reading_time': '15 minutes', 'summary': \"Google Gemini represents a breakthrough in AI capabilities, offering multimodal understanding that can revolutionize how we approach web scraping and data extraction. By combining Gemini's intelligent processing with modern web scraping APIs, you can build systems that don't just extract dataâ€”they understand it.\", 'tags': 'google gemini, ai, web scraping, data extraction, integrations', 'title': 'Google Gemini Web Scraping API: Complete Integration Guide'}, {'published_date': 'September 9, 2025', 'reading_time': '12 minutes', 'summary': \"LangChain has become the go-to framework for building sophisticated AI applications, providing powerful abstractions for document processing, embeddings, and retrieval systems. This guide shows how to combine LangChain's ecosystem with Supacrawler's intelligent web crawling and Supabase's vector storage to build a production-ready RAG system.\", 'tags': 'rag, langchain, supacrawler, pgvector, supabase, openai, integrations', 'title': 'Integrations: Building RAG with Supacrawler, LangChain, and Supabase pgvector for Enterprise'}, {'published_date': 'September 8, 2025', 'reading_time': '15 minutes', 'summary': \"LlamaIndex represents the cutting edge of RAG framework development, offering sophisticated indexing strategies, advanced query engines, and enterprise-grade features. This guide demonstrates how to build production-ready RAG systems using LlamaIndex's abstractions with Supacrawler's intelligent crawling and Supabase's scalable vector storage.\", 'tags': 'rag, llamaindex, supacrawler, pgvector, supabase, openai, integrations', 'title': 'Integrations: Build a RAG System with Supacrawler, LlamaIndex, and Supabase pgvector'}, {'published_date': 'September 7, 2025', 'reading_time': '8 minutes', 'summary': 'Build a production-ready RAG (Retrieval-Augmented Generation) system using Supacrawler for web data extraction, Supabase for vector storage, and OpenAI embeddings for semantic search. This guide provides a step-by-step approach to setting up and deploying your RAG system.', 'tags': 'rag, supacrawler, supabase, pgvector, openai, embeddings, integrations', 'title': 'Integrations: Build a simple RAG System with Supacrawler and Supabase pgvector using OpenAI embeddings'}], 'execution_time': 139194, 'input_tokens': 29439, 'output_tokens': 1549, 'pages_processed': 10, 'success': True, 'total_tokens': 30988, 'workflow_status': 'completed'}}, data={'parse_result': {'data': [{'published_date': 'August 28, 2025', 'reading_time': '10 minutes', 'summary': 'Retrieval-Augmented Generation (RAG) is a powerful technique that enhances Large Language Models (LLMs) by providing them with up-to-date, external knowledge. This guide provides a complete, end-to-end walkthrough of how to crawl an entire website to build a knowledge base, chunk the crawled content, embed the chunks into vectors, and store and query those vectors in a PostgreSQL database using the `pgvector` extension.', 'tags': 'rag, ai, llm, pgvector, supabase, langchain, llamaindex', 'title': 'Building a Production-Ready RAG Pipeline with Supacrawler and pgvector'}, {'published_date': 'August 26, 2025', 'reading_time': '5 minutes', 'summary': \"Watching websites for changes is a perfect use case for automation. Instead of manually checking or building brittle polling scripts, use Supacrawler's Watch API to schedule automatic checks, detect content changes with precision, and receive email notifications when changes occur.\", 'tags': 'watch api, automation, monitoring, website changes', 'title': 'Automate Monitoring Website Changes Automatically with Watch API'}, {'published_date': 'August 8, 2025', 'reading_time': '5 minutes', 'summary': \"Building a custom crawler can be a multi-week infrastructure project. This guide shows you how to crawl an entire documentation website with a single API call using the Supacrawler Jobs API. Get back a clean JSON object where each page's content is stored as markdown.\", 'tags': 'web crawling, crawl api, documentation, data extraction', 'title': 'Crawling a Full Documentation Site Using the Crawl Endpoint'}, {'published_date': 'September 12, 2025', 'reading_time': '7 minutes', 'summary': 'We benchmarked BeautifulSoup + requests against Supacrawler for static content scraping using identical retry and error handling logic. Supacrawler is purpose-built for LLMs and does significant additional processing: content cleaning, markdown conversion, metadata extraction, and noise removal.', 'tags': 'benchmarks, web scraping, beautifulsoup, performance', 'title': 'Supacrawler vs BeautifulSoup: Local Performance Benchmarks'}, {'published_date': 'September 12, 2025', 'reading_time': '10 minutes', 'summary': \"We benchmarked Playwright against Supacrawler for JavaScript-heavy web scraping using identical retry and error handling logic. Supacrawler uses a Go-based streaming worker pool vs Playwright's Python async sequential processing, leading to dramatic performance differences.\", 'tags': 'benchmarks, web scraping, playwright, performance, python', 'title': 'Supacrawler vs Playwright: Local Python Performance Benchmarks'}, {'published_date': 'September 12, 2025', 'reading_time': '10 minutes', 'summary': \"We benchmarked Selenium WebDriver against Supacrawler for JavaScript-heavy web scraping using identical retry and error handling logic. Supacrawler uses a Go-based streaming worker pool architecture vs Selenium's Python sequential processing, resulting in a significant performance advantage.\", 'tags': 'benchmarks, web scraping, selenium, performance, python', 'title': 'Supacrawler vs Selenium: Local Python Performance Benchmarks'}, {'published_date': 'September 11, 2025', 'reading_time': '15 minutes', 'summary': \"Google Gemini represents a breakthrough in AI capabilities, offering multimodal understanding that can revolutionize how we approach web scraping and data extraction. By combining Gemini's intelligent processing with modern web scraping APIs, you can build systems that don't just extract dataâ€”they understand it.\", 'tags': 'google gemini, ai, web scraping, data extraction, integrations', 'title': 'Google Gemini Web Scraping API: Complete Integration Guide'}, {'published_date': 'September 9, 2025', 'reading_time': '12 minutes', 'summary': \"LangChain has become the go-to framework for building sophisticated AI applications, providing powerful abstractions for document processing, embeddings, and retrieval systems. This guide shows how to combine LangChain's ecosystem with Supacrawler's intelligent web crawling and Supabase's vector storage to build a production-ready RAG system.\", 'tags': 'rag, langchain, supacrawler, pgvector, supabase, openai, integrations', 'title': 'Integrations: Building RAG with Supacrawler, LangChain, and Supabase pgvector for Enterprise'}, {'published_date': 'September 8, 2025', 'reading_time': '15 minutes', 'summary': \"LlamaIndex represents the cutting edge of RAG framework development, offering sophisticated indexing strategies, advanced query engines, and enterprise-grade features. This guide demonstrates how to build production-ready RAG systems using LlamaIndex's abstractions with Supacrawler's intelligent crawling and Supabase's scalable vector storage.\", 'tags': 'rag, llamaindex, supacrawler, pgvector, supabase, openai, integrations', 'title': 'Integrations: Build a RAG System with Supacrawler, LlamaIndex, and Supabase pgvector'}, {'published_date': 'September 7, 2025', 'reading_time': '8 minutes', 'summary': 'Build a production-ready RAG (Retrieval-Augmented Generation) system using Supacrawler for web data extraction, Supabase for vector storage, and OpenAI embeddings for semantic search. This guide provides a step-by-step approach to setting up and deploying your RAG system.', 'tags': 'rag, supacrawler, supabase, pgvector, openai, embeddings, integrations', 'title': 'Integrations: Build a simple RAG System with Supacrawler and Supabase pgvector using OpenAI embeddings'}], 'execution_time': 139194, 'input_tokens': 29439, 'output_tokens': 1549, 'pages_processed': 10, 'success': True, 'total_tokens': 30988, 'workflow_status': 'completed'}}, error=None)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parse_out = client.wait_for_parse(response.job_id)\n",
        "parse_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'parse_result': {'data': [{'published_date': 'August 28, 2025',\n",
              "    'reading_time': '10 minutes',\n",
              "    'summary': 'Retrieval-Augmented Generation (RAG) is a powerful technique that enhances Large Language Models (LLMs) by providing them with up-to-date, external knowledge. This guide provides a complete, end-to-end walkthrough of how to crawl an entire website to build a knowledge base, chunk the crawled content, embed the chunks into vectors, and store and query those vectors in a PostgreSQL database using the `pgvector` extension.',\n",
              "    'tags': 'rag, ai, llm, pgvector, supabase, langchain, llamaindex',\n",
              "    'title': 'Building a Production-Ready RAG Pipeline with Supacrawler and pgvector'},\n",
              "   {'published_date': 'August 26, 2025',\n",
              "    'reading_time': '5 minutes',\n",
              "    'summary': \"Watching websites for changes is a perfect use case for automation. Instead of manually checking or building brittle polling scripts, use Supacrawler's Watch API to schedule automatic checks, detect content changes with precision, and receive email notifications when changes occur.\",\n",
              "    'tags': 'watch api, automation, monitoring, website changes',\n",
              "    'title': 'Automate Monitoring Website Changes Automatically with Watch API'},\n",
              "   {'published_date': 'August 8, 2025',\n",
              "    'reading_time': '5 minutes',\n",
              "    'summary': \"Building a custom crawler can be a multi-week infrastructure project. This guide shows you how to crawl an entire documentation website with a single API call using the Supacrawler Jobs API. Get back a clean JSON object where each page's content is stored as markdown.\",\n",
              "    'tags': 'web crawling, crawl api, documentation, data extraction',\n",
              "    'title': 'Crawling a Full Documentation Site Using the Crawl Endpoint'},\n",
              "   {'published_date': 'September 12, 2025',\n",
              "    'reading_time': '7 minutes',\n",
              "    'summary': 'We benchmarked BeautifulSoup + requests against Supacrawler for static content scraping using identical retry and error handling logic. Supacrawler is purpose-built for LLMs and does significant additional processing: content cleaning, markdown conversion, metadata extraction, and noise removal.',\n",
              "    'tags': 'benchmarks, web scraping, beautifulsoup, performance',\n",
              "    'title': 'Supacrawler vs BeautifulSoup: Local Performance Benchmarks'},\n",
              "   {'published_date': 'September 12, 2025',\n",
              "    'reading_time': '10 minutes',\n",
              "    'summary': \"We benchmarked Playwright against Supacrawler for JavaScript-heavy web scraping using identical retry and error handling logic. Supacrawler uses a Go-based streaming worker pool vs Playwright's Python async sequential processing, leading to dramatic performance differences.\",\n",
              "    'tags': 'benchmarks, web scraping, playwright, performance, python',\n",
              "    'title': 'Supacrawler vs Playwright: Local Python Performance Benchmarks'},\n",
              "   {'published_date': 'September 12, 2025',\n",
              "    'reading_time': '10 minutes',\n",
              "    'summary': \"We benchmarked Selenium WebDriver against Supacrawler for JavaScript-heavy web scraping using identical retry and error handling logic. Supacrawler uses a Go-based streaming worker pool architecture vs Selenium's Python sequential processing, resulting in a significant performance advantage.\",\n",
              "    'tags': 'benchmarks, web scraping, selenium, performance, python',\n",
              "    'title': 'Supacrawler vs Selenium: Local Python Performance Benchmarks'},\n",
              "   {'published_date': 'September 11, 2025',\n",
              "    'reading_time': '15 minutes',\n",
              "    'summary': \"Google Gemini represents a breakthrough in AI capabilities, offering multimodal understanding that can revolutionize how we approach web scraping and data extraction. By combining Gemini's intelligent processing with modern web scraping APIs, you can build systems that don't just extract dataâ€”they understand it.\",\n",
              "    'tags': 'google gemini, ai, web scraping, data extraction, integrations',\n",
              "    'title': 'Google Gemini Web Scraping API: Complete Integration Guide'},\n",
              "   {'published_date': 'September 9, 2025',\n",
              "    'reading_time': '12 minutes',\n",
              "    'summary': \"LangChain has become the go-to framework for building sophisticated AI applications, providing powerful abstractions for document processing, embeddings, and retrieval systems. This guide shows how to combine LangChain's ecosystem with Supacrawler's intelligent web crawling and Supabase's vector storage to build a production-ready RAG system.\",\n",
              "    'tags': 'rag, langchain, supacrawler, pgvector, supabase, openai, integrations',\n",
              "    'title': 'Integrations: Building RAG with Supacrawler, LangChain, and Supabase pgvector for Enterprise'},\n",
              "   {'published_date': 'September 8, 2025',\n",
              "    'reading_time': '15 minutes',\n",
              "    'summary': \"LlamaIndex represents the cutting edge of RAG framework development, offering sophisticated indexing strategies, advanced query engines, and enterprise-grade features. This guide demonstrates how to build production-ready RAG systems using LlamaIndex's abstractions with Supacrawler's intelligent crawling and Supabase's scalable vector storage.\",\n",
              "    'tags': 'rag, llamaindex, supacrawler, pgvector, supabase, openai, integrations',\n",
              "    'title': 'Integrations: Build a RAG System with Supacrawler, LlamaIndex, and Supabase pgvector'},\n",
              "   {'published_date': 'September 7, 2025',\n",
              "    'reading_time': '8 minutes',\n",
              "    'summary': 'Build a production-ready RAG (Retrieval-Augmented Generation) system using Supacrawler for web data extraction, Supabase for vector storage, and OpenAI embeddings for semantic search. This guide provides a step-by-step approach to setting up and deploying your RAG system.',\n",
              "    'tags': 'rag, supacrawler, supabase, pgvector, openai, embeddings, integrations',\n",
              "    'title': 'Integrations: Build a simple RAG System with Supacrawler and Supabase pgvector using OpenAI embeddings'}],\n",
              "  'execution_time': 139194,\n",
              "  'input_tokens': 29439,\n",
              "  'output_tokens': 1549,\n",
              "  'pages_processed': 10,\n",
              "  'success': True,\n",
              "  'total_tokens': 30988,\n",
              "  'workflow_status': 'completed'}}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parse_out.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parse with JSON Schema\n",
        "\n",
        "You can provide a structured JSON schema to get precise data extraction:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse with structured schema\n",
        "schema = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"title\": {\"type\": \"string\"},\n",
        "        \"content\": {\"type\": \"string\"},\n",
        "        \"links\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\"type\": \"string\"}\n",
        "        }\n",
        "    },\n",
        "    \"required\": [\"title\", \"content\"]\n",
        "}\n",
        "\n",
        "response = client.parse(\n",
        "    \"Extract the page title, main content, and any links from https://httpbin.org/html\",\n",
        "    schema=schema,\n",
        "    output_format=\"json\"\n",
        ")\n",
        "\n",
        "print(\"Structured extraction:\")\n",
        "print(f\"Success: {response.success}\")\n",
        "if response.has_data:\n",
        "    import json\n",
        "    print(\"Extracted data:\")\n",
        "    if isinstance(response.data, dict):\n",
        "        print(json.dumps(response.data, indent=2))\n",
        "    else:\n",
        "        print(response.data)\n",
        "elif response.error:\n",
        "    print(f\"Error: {response.error}\")\n",
        "else:\n",
        "    print(\"No data available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Different Output Formats\n",
        "\n",
        "The Parse API supports multiple output formats:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CSV format example\n",
        "csv_response = client.parse(\n",
        "    \"Extract any information from https://httpbin.org/html in a table format\",\n",
        "    output_format=\"csv\"\n",
        ")\n",
        "\n",
        "print(\"CSV Output:\")\n",
        "print(f\"Success: {csv_response.success}\")\n",
        "if csv_response.has_data:\n",
        "    print(\"CSV Data:\")\n",
        "    print(csv_response.data)\n",
        "elif csv_response.error:\n",
        "    print(f\"Error: {csv_response.error}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Markdown format example  \n",
        "md_response = client.parse(\n",
        "    \"Summarize the content from https://httpbin.org/html\",\n",
        "    output_format=\"markdown\"\n",
        ")\n",
        "\n",
        "print(\"Markdown Output:\")\n",
        "print(f\"Success: {md_response.success}\")\n",
        "if md_response.has_data:\n",
        "    print(\"Markdown Data:\")\n",
        "    print(md_response.data)\n",
        "elif md_response.error:\n",
        "    print(f\"Error: {md_response.error}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
