{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Selenium vs Supacrawler: A Practical Comparison\n",
        "\n",
        "This notebook provides a practical comparison between Selenium, a popular browser automation tool, and Supacrawler API, a cloud-based web scraping solution.\n",
        "\n",
        "We'll compare these tools across several dimensions:\n",
        "1. Setup and installation\n",
        "2. Basic web scraping\n",
        "3. Performance benchmarks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Installation\n",
        "# Uncomment and run these commands to install the required packages\n",
        "\n",
        "# !pip install selenium webdriver-manager\n",
        "# !pip install supacrawler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import time\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Basic Usage\n",
        "\n",
        "Let's compare how to set up and use Selenium and Supacrawler for a simple task: navigating to a webpage and getting its title.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Selenium - Basic Setup\n",
        "def selenium_basic(url):\n",
        "    from selenium import webdriver\n",
        "    from selenium.webdriver.chrome.service import Service\n",
        "    from webdriver_manager.chrome import ChromeDriverManager\n",
        "    from selenium.webdriver.chrome.options import Options\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Configure Chrome options\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--headless\")\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    \n",
        "    # Setup Chrome driver\n",
        "    service = Service(ChromeDriverManager().install())\n",
        "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "    \n",
        "    # Navigate to URL\n",
        "    driver.get(url)\n",
        "    \n",
        "    # Wait for page to load (simple implementation)\n",
        "    time.sleep(2)\n",
        "    \n",
        "    # Get page title\n",
        "    title = driver.title\n",
        "    \n",
        "    # Close browser\n",
        "    driver.quit()\n",
        "    \n",
        "    end_time = time.time()\n",
        "    setup_time = end_time - start_time\n",
        "    \n",
        "    return {\n",
        "        \"title\": title,\n",
        "        \"setup_time\": setup_time\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Supacrawler - Basic Setup\n",
        "def supacrawler_basic(url):\n",
        "    from supacrawler import SupacrawlerClient\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Initialize client\n",
        "    client = SupacrawlerClient(api_key=os.environ.get('SUPACRAWLER_API_KEY', 'YOUR_API_KEY'))\n",
        "    \n",
        "    # Make API request\n",
        "    response = client.scrape(url=url, render_js=True)\n",
        "    \n",
        "    # Get page title\n",
        "    title = response.metadata.title if response.metadata else \"No title\"\n",
        "    \n",
        "    end_time = time.time()\n",
        "    setup_time = end_time - start_time\n",
        "    \n",
        "    return {\n",
        "        \"title\": title,\n",
        "        \"setup_time\": setup_time\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run basic comparison\n",
        "def compare_basic(url):\n",
        "    results = {}\n",
        "    \n",
        "    print(f\"Testing basic setup for URL: {url}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    try:\n",
        "        print(\"Running Selenium basic setup...\")\n",
        "        results[\"selenium\"] = selenium_basic(url)\n",
        "        print(f\"Title: {results['selenium']['title']}\")\n",
        "        print(f\"Setup time: {results['selenium']['setup_time']:.2f} seconds\")\n",
        "    except Exception as e:\n",
        "        print(f\"Selenium error: {e}\")\n",
        "        results[\"selenium\"] = {\"title\": \"Error\", \"setup_time\": 0}\n",
        "    \n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    try:\n",
        "        print(\"Running Supacrawler basic setup...\")\n",
        "        results[\"supacrawler\"] = supacrawler_basic(url)\n",
        "        print(f\"Title: {results['supacrawler']['title']}\")\n",
        "        print(f\"Setup time: {results['supacrawler']['setup_time']:.2f} seconds\")\n",
        "    except Exception as e:\n",
        "        print(f\"Supacrawler error: {e}\")\n",
        "        results[\"supacrawler\"] = {\"title\": \"Error\", \"setup_time\": 0}\n",
        "    \n",
        "    print(\"-\" * 50)\n",
        "    return results\n",
        "\n",
        "# Run comparison with example.com\n",
        "# basic_results = compare_basic(\"https://example.com\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize basic comparison\n",
        "def plot_basic_times(results):\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    tools = list(results.keys())\n",
        "    times = [results[tool][\"setup_time\"] for tool in tools]\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    bars = plt.bar(tools, times)\n",
        "    \n",
        "    # Add labels and title\n",
        "    plt.xlabel('Tool')\n",
        "    plt.ylabel('Setup Time (seconds)')\n",
        "    plt.title('Browser Automation Tools: Setup Time Comparison')\n",
        "    \n",
        "    # Add values on top of bars\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
        "                 f'{height:.2f}s',\n",
        "                 ha='center', va='bottom')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize the results\n",
        "# plot_basic_times(basic_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Web Scraping Comparison\n",
        "\n",
        "Now let's compare how each tool handles extracting data from a webpage with JavaScript content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample JavaScript-heavy page URL\n",
        "JS_HEAVY_URL = \"https://quotes.toscrape.com/js/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Selenium - Scrape JavaScript Content\n",
        "def selenium_scrape_js(url):\n",
        "    from selenium import webdriver\n",
        "    from selenium.webdriver.chrome.service import Service\n",
        "    from webdriver_manager.chrome import ChromeDriverManager\n",
        "    from selenium.webdriver.chrome.options import Options\n",
        "    from selenium.webdriver.common.by import By\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Configure Chrome options\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--headless\")\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    \n",
        "    # Setup Chrome driver\n",
        "    service = Service(ChromeDriverManager().install())\n",
        "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "    \n",
        "    # Navigate to URL\n",
        "    driver.get(url)\n",
        "    \n",
        "    # Wait for JavaScript to load content\n",
        "    time.sleep(3)\n",
        "    \n",
        "    # Extract quotes data\n",
        "    quotes = []\n",
        "    quote_elements = driver.find_elements(By.CLASS_NAME, \"quote\")\n",
        "    \n",
        "    for quote_element in quote_elements:\n",
        "        quote_text = quote_element.find_element(By.CLASS_NAME, \"text\").text\n",
        "        quote_author = quote_element.find_element(By.CLASS_NAME, \"author\").text\n",
        "        quotes.append({\n",
        "            \"text\": quote_text,\n",
        "            \"author\": quote_author\n",
        "        })\n",
        "    \n",
        "    # Close browser\n",
        "    driver.quit()\n",
        "    \n",
        "    end_time = time.time()\n",
        "    scrape_time = end_time - start_time\n",
        "    \n",
        "    return {\n",
        "        \"quotes\": quotes,\n",
        "        \"count\": len(quotes),\n",
        "        \"scrape_time\": scrape_time\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Supacrawler - Scrape JavaScript Content\n",
        "def supacrawler_scrape_js(url):\n",
        "    from supacrawler import SupacrawlerClient\n",
        "    import re\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Initialize client\n",
        "    client = SupacrawlerClient(api_key=os.environ.get('SUPACRAWLER_API_KEY', 'YOUR_API_KEY'))\n",
        "    \n",
        "    # Make API request with JavaScript rendering\n",
        "    response = client.scrape(url=url, render_js=True)\n",
        "    \n",
        "    # Extract quotes data using the HTML content\n",
        "    quotes = []\n",
        "    \n",
        "    if response.html:\n",
        "        # Simple regex-based extraction (in a real scenario, you might use BeautifulSoup)\n",
        "        quote_pattern = r'<div class=\"quote\".*?<span class=\"text\".*?>(.*?)</span>.*?<small class=\"author\">(.*?)</small>'\n",
        "        matches = re.findall(quote_pattern, response.html, re.DOTALL)\n",
        "        \n",
        "        for match in matches:\n",
        "            quotes.append({\n",
        "                \"text\": match[0].strip(),\n",
        "                \"author\": match[1].strip()\n",
        "            })\n",
        "    \n",
        "    end_time = time.time()\n",
        "    scrape_time = end_time - start_time\n",
        "    \n",
        "    return {\n",
        "        \"quotes\": quotes,\n",
        "        \"count\": len(quotes),\n",
        "        \"scrape_time\": scrape_time\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run JavaScript scraping comparison\n",
        "def compare_js_scraping(url):\n",
        "    results = {}\n",
        "    \n",
        "    print(f\"Testing JavaScript scraping for URL: {url}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    try:\n",
        "        print(\"Running Selenium JavaScript scraping...\")\n",
        "        results[\"selenium\"] = selenium_scrape_js(url)\n",
        "        print(f\"Quotes found: {results['selenium']['count']}\")\n",
        "        if results['selenium']['quotes']:\n",
        "            print(f\"First quote: \\\"{results['selenium']['quotes'][0]['text']}\\\" - {results['selenium']['quotes'][0]['author']}\")\n",
        "        print(f\"Scrape time: {results['selenium']['scrape_time']:.2f} seconds\")\n",
        "    except Exception as e:\n",
        "        print(f\"Selenium error: {e}\")\n",
        "        results[\"selenium\"] = {\"quotes\": [], \"count\": 0, \"scrape_time\": 0}\n",
        "    \n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    try:\n",
        "        print(\"Running Supacrawler JavaScript scraping...\")\n",
        "        results[\"supacrawler\"] = supacrawler_scrape_js(url)\n",
        "        print(f\"Quotes found: {results['supacrawler']['count']}\")\n",
        "        if results['supacrawler']['quotes']:\n",
        "            print(f\"First quote: \\\"{results['supacrawler']['quotes'][0]['text']}\\\" - {results['supacrawler']['quotes'][0]['author']}\")\n",
        "        print(f\"Scrape time: {results['supacrawler']['scrape_time']:.2f} seconds\")\n",
        "    except Exception as e:\n",
        "        print(f\"Supacrawler error: {e}\")\n",
        "        results[\"supacrawler\"] = {\"quotes\": [], \"count\": 0, \"scrape_time\": 0}\n",
        "    \n",
        "    print(\"-\" * 50)\n",
        "    return results\n",
        "\n",
        "# Uncomment to run the comparison\n",
        "# js_results = compare_js_scraping(JS_HEAVY_URL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize JavaScript scraping comparison\n",
        "def plot_js_comparison(results):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    \n",
        "    tools = list(results.keys())\n",
        "    times = [results[tool][\"scrape_time\"] for tool in tools]\n",
        "    counts = [results[tool][\"count\"] for tool in tools]\n",
        "    \n",
        "    # Set up the figure with two subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # Plot times\n",
        "    bars1 = ax1.bar(tools, times)\n",
        "    ax1.set_xlabel('Tool')\n",
        "    ax1.set_ylabel('Time (seconds)')\n",
        "    ax1.set_title('JavaScript Scraping Time')\n",
        "    \n",
        "    # Add values on top of bars\n",
        "    for bar in bars1:\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
        "                 f'{height:.2f}s',\n",
        "                 ha='center', va='bottom')\n",
        "    \n",
        "    # Plot quote counts\n",
        "    bars2 = ax2.bar(tools, counts)\n",
        "    ax2.set_xlabel('Tool')\n",
        "    ax2.set_ylabel('Quotes Count')\n",
        "    ax2.set_title('Quotes Retrieved')\n",
        "    \n",
        "    # Add values on top of bars\n",
        "    for bar in bars2:\n",
        "        height = bar.get_height()\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
        "                 f'{int(height)}',\n",
        "                 ha='center', va='bottom')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Uncomment to visualize the results after running the comparison\n",
        "# plot_js_comparison(js_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature Comparison\n",
        "\n",
        "Let's compare the key features of Selenium and Supacrawler.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison table\n",
        "def create_comparison_table():\n",
        "    import pandas as pd\n",
        "    \n",
        "    # Create comparison data\n",
        "    data = {\n",
        "        \"Feature\": [\n",
        "            \"Languages\", \n",
        "            \"Browsers\", \n",
        "            \"Performance\", \n",
        "            \"Reliability\", \n",
        "            \"Setup Complexity\", \n",
        "            \"Maintenance\", \n",
        "            \"Best For\"\n",
        "        ],\n",
        "        \"Selenium\": [\n",
        "            \"Java, Python, C#, Ruby, JavaScript\",\n",
        "            \"Chrome, Firefox, Edge, Safari\",\n",
        "            \"★★★☆☆\",\n",
        "            \"★★★☆☆\",\n",
        "            \"★★★★☆\",\n",
        "            \"High\",\n",
        "            \"Legacy systems, enterprise\"\n",
        "        ],\n",
        "        \"Supacrawler API\": [\n",
        "            \"JavaScript, Python, REST API\",\n",
        "            \"Chrome (headless)\",\n",
        "            \"★★★★★\",\n",
        "            \"★★★★★\",\n",
        "            \"★☆☆☆☆\",\n",
        "            \"None\",\n",
        "            \"Production scraping at scale\"\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Display comparison table\n",
        "# create_comparison_table()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "Based on our comparison, here are the key takeaways:\n",
        "\n",
        "1. **Selenium** is a well-established browser automation framework with broad browser support, but comes with higher maintenance overhead and slower performance.\n",
        "\n",
        "2. **Supacrawler API** provides a simpler implementation with no infrastructure management, making it ideal for production scraping at scale with minimal development effort.\n",
        "\n",
        "For most web scraping needs, Supacrawler API offers a better developer experience than Selenium, especially when you need to focus on extracting data rather than managing browser automation infrastructure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature Comparison\n",
        "\n",
        "Let's compare the key features of Selenium and Supacrawler.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
